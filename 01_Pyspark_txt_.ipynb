{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01 Pyspark - txt .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z91iSn39PQnv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfgrf/PySpark-Google-Colab/blob/main/01_Pyspark_txt_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91iSn39PQnv"
      },
      "source": [
        "#Download/instalação dependências Spark/Pyspark\n",
        "*primeira run aprox 60segundos\n",
        "\n",
        "1.   apt-get openjdk-8-jdk-headless\n",
        "2.   download spark-3.2.0-bin-hadoop3.2.tgz\n",
        "3.   unzip spark-3.2.0-bin-hadoop3.2    \n",
        "4.   download sherlock.txt\n",
        "5.   pip install findspark (pyspark)\n",
        "6.   set JAVA_HOME e SPARK_HOME paths\n",
        "7.   cria variavel \"spark\" como sparkSession\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6i4txWWMw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d50ef7f-d9a7-4b22-cb4d-662da6e43fce"
      },
      "source": [
        "%%bash\n",
        "#***IFs apenas para o notebook não executar novamente em caso de run all cells.\n",
        "\n",
        "#verifica se openjdk está instalado\n",
        "if (dpkg -l | grep -qw openjdk-8-jdk-headless) then \n",
        "  echo \"Ja instalado - openjdk-8-jdk-headless\" \n",
        "else \n",
        "  apt-get install openjdk-8-jdk-headless -qq > /dev/null | echo \"openjdk-8-jdk-headless - instalado com sucesso\" \n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless - instalado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWPErew7HHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5758076-3093-4892-8601-6a5c9424d708"
      },
      "source": [
        "%%bash\n",
        "#download spark-hadoop\n",
        "if [ -f \"spark-3.2.0-bin-hadoop3.2.tgz\" ]; then\n",
        "  echo \"Ja baixado - spark-3.2.0-bin-hadoop3.2.tgz\"\n",
        "else \n",
        "  wget -q  https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz | echo \"spark-3.2.0-bin-hadoop3.2.tgz - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.2.0-bin-hadoop3.2.tgz - baixado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lnOz9fC7HQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c425aa49-ed44-4ef5-fc9a-1c8c96facd7a"
      },
      "source": [
        "%%bash\n",
        "#extrair spark-hadoop\n",
        "if [ -d \"spark-3.2.0-bin-hadoop3.2\" ]; then\n",
        "  echo \"Ja descompactado - Pasta spark-3.2.0-bin-hadoop3.2\"\n",
        "else \n",
        "  tar xf spark-3.2.0-bin-hadoop3.2.tgz | echo \"spark-3.2.0-bin-hadoop3.2.tgz - descompactado com sucesso\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.2.0-bin-hadoop3.2.tgz - descompactado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIRCV5z7Hb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda0e4be-57ed-4986-a763-4f391e6e3bf8"
      },
      "source": [
        "%%bash\n",
        "#download sherlock.txt\n",
        "if [ -f \"/content/sample_data/sherlock.txt\" ]; then\n",
        "  echo \"Ja baixado - sherlock.txt\"\n",
        "else \n",
        "  wget -q -O /content/sample_data/sherlock.txt https://www.gutenberg.org/files/1661/1661-0.txt | echo \"sherlock.txt - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sherlock.txt - baixado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht_iWNwOPqrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878db73d-7139-4e67-cbe0-ae7bd040d875"
      },
      "source": [
        "#usa folder do spark como lib pyspark\n",
        "try:\n",
        "    findspark\n",
        "except NameError:\n",
        "    !pip install -q findspark\n",
        "    import findspark\n",
        "    findspark.init('spark-3.2.0-bin-hadoop3.2')\n",
        "    print(\"findspark - instalado com sucesso \")\n",
        "else:\n",
        "    print(\"já instalado - findspark\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "findspark - instalado com sucesso \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvBZvULP8bV"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcDnw-MsFiQU"
      },
      "source": [
        "#cria spark session \n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhYFUL3jxygD"
      },
      "source": [
        "# TOP 10 palavras com RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEClBJC9w-n_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea11e54a-676d-447f-ce71-80c04a19ab94"
      },
      "source": [
        "#Exemplo1 com algumas funções Spark\n",
        "#atencao para unicodes e como limpar caracteres especiais\n",
        "import re\n",
        "\n",
        "#wholeTextFiles lê o arquivo inteiro em uma tupla. lista[(path,texto_inteiro)] \n",
        "rddExemplo1 = spark.sparkContext.wholeTextFiles('/content/sample_data/sherlock.txt')\n",
        "\n",
        "#Trata o texto como no exemplo da documentacao: \\s+\n",
        "rddExemplo1 = rddExemplo1.map(lambda x : re.sub(\"\\s+\",\" \",x[1]))\n",
        "\n",
        "#Split por espaço em branco\n",
        "rddExemplo1 = rddExemplo1.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "#Adiciona coluna com valor 1 para reduce (exemplo MRJob)\n",
        "rddExemplo1 = rddExemplo1.map(lambda x: (x,1)) \n",
        "rddExemplo1.take(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\\ufeffThe', 1), ('Project', 1), ('Gutenberg', 1), ('eBook', 1), ('of', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X33OFau2ydi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733615f0-019a-49aa-c56d-9150b04c0e14"
      },
      "source": [
        "#Exemplo1 - Solucao1\n",
        "# RDD final com ReduceByKey (reduz keys iguais e soma values)\n",
        "solucao1 = rddExemplo1.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#TOP 10\n",
        "solucao1.sortBy(lambda x: x[1],ascending=False).take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBHB6EVw-g0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc08ecc-5669-43a5-b6bb-39a51b4fcbad"
      },
      "source": [
        "#Exemplo1 - Solucao2\n",
        "#coleta rdd em lista\n",
        "from operator import itemgetter\n",
        "solucao2 = rddExemplo1.groupByKey().mapValues(len).collect()\n",
        "\n",
        "#top10\n",
        "solucao2.sort(key=itemgetter(1),reverse=True)\n",
        "solucao2[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SoQh-Stx4wC"
      },
      "source": [
        "# TOP 10 palavras com DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBIdhB76RXy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162a6f5a-d1b2-4928-d1e9-80e6dd1d128c"
      },
      "source": [
        "#Exemplo da documentacao Spark - DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "dfExemplo2 =spark.read.format('text').load('/content/sample_data/sherlock.txt')\n",
        "\n",
        "wordCounts = dfExemplo2.select(explode(split(dfExemplo2.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
        "\n",
        "#top 10\n",
        "wordCounts.filter(wordCounts['word']!=\"\").orderBy(['count'],ascending=False).show(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 5412|\n",
            "| and| 2794|\n",
            "|  of| 2724|\n",
            "|  to| 2702|\n",
            "|   a| 2575|\n",
            "|   I| 2533|\n",
            "|  in| 1706|\n",
            "|that| 1557|\n",
            "| was| 1361|\n",
            "| his| 1096|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VbBC5AfzRNT"
      },
      "source": [
        "# TOP 10 palavras com DataFrame + SQLquery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQgWgaeQzWdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd266d09-aa4e-4aa1-a41a-cd38d002d598"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "rddExemplo3 =spark.read.text('/content/sample_data/sherlock.txt').rdd\n",
        "\n",
        "#Split RDD nos espaços em branco\n",
        "rddExemplo3 = rddExemplo3.flatMap(lambda x: x[0].split(\" \"))\n",
        "\n",
        "#add nome Coluna para converter em Dataframe\n",
        "dfExemplo3 = rddExemplo3.map(Row(\"palavra\")).toDF()\n",
        "\n",
        "#Precisa criar tabela/view temporaria para executar queries SQL\n",
        "dfExemplo3.createOrReplaceTempView('tabelaTemp')\n",
        "\n",
        "#Query \n",
        "queryTop10 = \"\"\"SELECT palavra,\n",
        "                       COUNT(*) as qnt\n",
        "                  FROM tabelaTemp\n",
        "                 WHERE palavra <> \"\"           \n",
        "              GROUP BY palavra\n",
        "              ORDER BY 2 DESC\n",
        "                 LIMIT 10\n",
        "             \"\"\"\n",
        "\n",
        "#Executa query\n",
        "spark.sql(queryTop10).show()\n",
        "\n",
        "#dropa tabela temporaria\n",
        "spark.catalog.dropTempView('tabelaTemp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|palavra| qnt|\n",
            "+-------+----+\n",
            "|    the|5412|\n",
            "|    and|2794|\n",
            "|     of|2724|\n",
            "|     to|2702|\n",
            "|      a|2575|\n",
            "|      I|2533|\n",
            "|     in|1706|\n",
            "|   that|1557|\n",
            "|    was|1361|\n",
            "|    his|1096|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTqIgdshoSfP"
      },
      "source": [
        "#TOP 10 palavras com DataFrame + Python list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uUzC-J3CyF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228895d1-2877-4836-9b8e-8e675b213d25"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#leitura .txt\n",
        "#Dataframe com 1 COL e cada ROW sendo uma linha do .txt\n",
        "dfExemplo4 =spark.read.text('/content/sample_data/sherlock.txt')\n",
        "\n",
        "\n",
        "#coluna value (nome default) do Dataframe para python list[]\n",
        "listLivro = list(dfExemplo4.select(\"value\").toPandas()[\"value\"])\n",
        "\n",
        "\n",
        "#Formata list[linhaTxt1,...,linhaTxtN]\n",
        "#   para list[palavra1,...,palavraN]\n",
        "def listPalavras(listLivro):\n",
        "  resultado = []\n",
        "  for paragrafo in listLivro:\n",
        "   [resultado.append(palavra) for palavra in paragrafo.split()]\n",
        "  return resultado    \n",
        "\n",
        "#Conta ocorrencias com collections Counter\n",
        "contaPalavra1 = Counter(listPalavras(listLivro))\n",
        "\n",
        "#TOP 10\n",
        "listaTOP10 = contaPalavra1.most_common(10)\n",
        "listaTOP10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxDQIJQHX5bj"
      },
      "source": [
        "# Lista -> rdd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmV2jIchTxVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7700a11-2663-4afe-d64a-bf99989063d6"
      },
      "source": [
        "#python list -> spark dataframe\n",
        "rdd = spark.sparkContext.parallelize(listaTOP10)\n",
        "\n",
        "rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fAiu_jAX-cv"
      },
      "source": [
        "# Lista -> Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx_CMJ_rYN0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7073a061-8002-4cec-c956-46a1db6a75b3"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "#novo schema\n",
        "schema = StructType([\n",
        "    StructField('palavra', StringType(), True),\n",
        "    StructField('qnt', StringType(), True)\n",
        "])\n",
        "#monta df\n",
        "df = spark.createDataFrame(listaTOP10,schema)\n",
        "df.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|palavra| qnt|\n",
            "+-------+----+\n",
            "|    the|5412|\n",
            "|    and|2794|\n",
            "|     of|2724|\n",
            "|     to|2702|\n",
            "|      a|2575|\n",
            "|      I|2533|\n",
            "|     in|1706|\n",
            "|   that|1557|\n",
            "|    was|1361|\n",
            "|    his|1096|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4IVHLZoZFBn"
      },
      "source": [
        "# Dataframe -> .txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y28qY1y8VAtu"
      },
      "source": [
        "#Concatena colunas selecionadas e cria novo df com apenas 1 coluna\n",
        "df.createOrReplaceTempView('tabelaConc')\n",
        "df1coluna = spark.sql(\"\"\"SELECT CONCAT(palavra, ' ', qnt) palavra_e_qnt FROM tabelaConc\"\"\")\n",
        "spark.catalog.dropTempView('tabelaConc')\n",
        "\n",
        "#salva df em .txt\n",
        "df1coluna.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save('/content/sample_data/OUTPUT-TOP10sherlock')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrN9d691zKd-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}