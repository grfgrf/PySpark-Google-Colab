{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01 Pyspark - txt .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfgrf/PySpark-Google-Colab/blob/main/01_Pyspark_txt_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91iSn39PQnv"
      },
      "source": [
        "#Download/instalação dependências Spark/Pyspark\n",
        "*primeira run aprox 60segundos\n",
        "\n",
        "1.   apt-get openjdk-8-jdk-headless\n",
        "2.   download spark-3.2.0-bin-hadoop3.2.tgz\n",
        "3.   unzip spark-3.2.0-bin-hadoop3.2    \n",
        "4.   download sherlock.txt\n",
        "5.   pip install findspark (pyspark)\n",
        "6.   set JAVA_HOME e SPARK_HOME paths\n",
        "7.   cria variavel \"spark\" como sparkSession\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6i4txWWMw-"
      },
      "source": [
        "%%bash\n",
        "#***IFs apenas para o notebook não executar novamente em caso de run all cells.\n",
        "\n",
        "#verifica se openjdk está instalado\n",
        "if ! (dpkg -l | grep -qw openjdk-8-jdk-headless) then \n",
        "  apt-get install openjdk-8-jdk-headless -qq > /dev/null #| echo \"openjdk-8-jdk-headless - instalado com sucesso\" \n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWPErew7HHg"
      },
      "source": [
        "%%bash\n",
        "#download spark-hadoop\n",
        "if ! [ -f \"spark-3.2.0-bin-hadoop3.2.tgz\" ]; then\n",
        "  wget -q  https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz #| echo \"spark-3.2.0-bin-hadoop3.2.tgz - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lnOz9fC7HQ2"
      },
      "source": [
        "%%bash\n",
        "#extrair spark-hadoop\n",
        "if ! [ -d \"spark-3.2.0-bin-hadoop3.2\" ]; then\n",
        "  tar xf spark-3.2.0-bin-hadoop3.2.tgz #| echo \"spark-3.2.0-bin-hadoop3.2.tgz - descompactado com sucesso\"\n",
        "fi"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIRCV5z7Hb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53948188-4815-4f40-a013-a9f77c373b73"
      },
      "source": [
        "%%bash\n",
        "#download sherlock.txt\n",
        "if ! [ -f \"/content/sample_data/sherlock.txt\" ]; then\n",
        "  wget -q -O /content/sample_data/sherlock.txt https://www.gutenberg.org/files/1661/1661-0.txt #| echo \"sherlock.txt - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sherlock.txt - baixado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht_iWNwOPqrt"
      },
      "source": [
        "#usa folder do spark como lib pyspark\n",
        "try:\n",
        "    findspark\n",
        "except NameError:\n",
        "    !pip install -q findspark\n",
        "    import findspark\n",
        "    findspark.init('spark-3.2.0-bin-hadoop3.2')\n",
        "    #print(\"findspark - instalado com sucesso \")\n",
        "#else:\n",
        "    #print(\"já instalado - findspark\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvBZvULP8bV"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcDnw-MsFiQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "04fc83a3-c242-4b32-bb41-5aa8c7f6683f"
      },
      "source": [
        "#cria spark session \n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://27e170f818f8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc56f6c5990>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhYFUL3jxygD"
      },
      "source": [
        "# TOP 10 palavras com RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEClBJC9w-n_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf83a4f-e574-456f-f589-e2f9f029650f"
      },
      "source": [
        "#Exemplo1 com algumas funções Spark\n",
        "#atencao para unicodes e como limpar caracteres especiais\n",
        "import re\n",
        "\n",
        "#wholeTextFiles lê o arquivo inteiro em uma tupla. lista[(path,texto_inteiro)] \n",
        "rddExemplo1 = spark.sparkContext.wholeTextFiles('/content/sample_data/sherlock.txt')\n",
        "\n",
        "#Trata o texto como no exemplo da documentacao: \\s+\n",
        "rddExemplo1 = rddExemplo1.map(lambda x : re.sub(\"\\s+\",\" \",x[1]))\n",
        "\n",
        "#Split por espaço em branco\n",
        "rddExemplo1 = rddExemplo1.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "#Adiciona coluna com valor 1 para reduce (exemplo MRJob)\n",
        "rddExemplo1 = rddExemplo1.map(lambda x: (x,1)) \n",
        "rddExemplo1.take(5)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\\ufeffThe', 1), ('Project', 1), ('Gutenberg', 1), ('eBook', 1), ('of', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X33OFau2ydi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d52ff02-3997-4d19-b6cc-e4d178721687"
      },
      "source": [
        "#Exemplo1 - Solucao1\n",
        "# RDD final com ReduceByKey (reduz keys iguais e soma values)\n",
        "solucao1 = rddExemplo1.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#TOP 10\n",
        "solucao1.sortBy(lambda x: x[1],ascending=False).take(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBHB6EVw-g0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d934b3-6bad-4e61-e89d-cf0304a632cb"
      },
      "source": [
        "#Exemplo1 - Solucao2\n",
        "#coleta rdd em lista\n",
        "from operator import itemgetter\n",
        "solucao2 = rddExemplo1.groupByKey().mapValues(len).collect()\n",
        "\n",
        "#top10\n",
        "solucao2.sort(key=itemgetter(1),reverse=True)\n",
        "solucao2[0:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SoQh-Stx4wC"
      },
      "source": [
        "# TOP 10 palavras com DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBIdhB76RXy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09851df5-14f9-4ccd-fb59-5bc857ed9eee"
      },
      "source": [
        "#Exemplo da documentacao Spark - DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "dfExemplo2 =spark.read.format('text').load('/content/sample_data/sherlock.txt')\n",
        "\n",
        "wordCounts = dfExemplo2.select(explode(split(dfExemplo2.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
        "\n",
        "#top 10\n",
        "wordCounts.filter(wordCounts['word']!=\"\").orderBy(['count'],ascending=False).show(10)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 5412|\n",
            "| and| 2794|\n",
            "|  of| 2724|\n",
            "|  to| 2702|\n",
            "|   a| 2575|\n",
            "|   I| 2533|\n",
            "|  in| 1706|\n",
            "|that| 1557|\n",
            "| was| 1361|\n",
            "| his| 1096|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VbBC5AfzRNT"
      },
      "source": [
        "# TOP 10 palavras com DataFrame + SQLquery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQgWgaeQzWdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542c3a71-636d-42f8-c827-70edbc3a6245"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "rddExemplo3 =spark.read.text('/content/sample_data/sherlock.txt').rdd\n",
        "\n",
        "#Split RDD nos espaços em branco\n",
        "rddExemplo3 = rddExemplo3.flatMap(lambda x: x[0].split(\" \"))\n",
        "\n",
        "#add nome Coluna para converter em Dataframe\n",
        "dfExemplo3 = rddExemplo3.map(Row(\"palavra\")).toDF()\n",
        "\n",
        "#Precisa criar tabela/view temporaria para executar queries SQL\n",
        "dfExemplo3.createOrReplaceTempView('tabelaTemp')\n",
        "\n",
        "#Query \n",
        "queryTop10 = \"\"\"SELECT palavra,\n",
        "                       COUNT(*) as qnt\n",
        "                  FROM tabelaTemp\n",
        "                 WHERE palavra <> \"\"           \n",
        "              GROUP BY palavra\n",
        "              ORDER BY 2 DESC\n",
        "                 LIMIT 10\n",
        "             \"\"\"\n",
        "\n",
        "#Executa query\n",
        "spark.sql(queryTop10).show()\n",
        "\n",
        "#dropa tabela temporaria\n",
        "spark.catalog.dropTempView('tabelaTemp')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|palavra| qnt|\n",
            "+-------+----+\n",
            "|    the|5412|\n",
            "|    and|2794|\n",
            "|     of|2724|\n",
            "|     to|2702|\n",
            "|      a|2575|\n",
            "|      I|2533|\n",
            "|     in|1706|\n",
            "|   that|1557|\n",
            "|    was|1361|\n",
            "|    his|1096|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTqIgdshoSfP"
      },
      "source": [
        "#TOP 10 palavras com DataFrame + Python list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uUzC-J3CyF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d72d73e-11af-48b8-ce8d-fa73a40f1722"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#leitura .txt\n",
        "#Dataframe com 1 COL e cada ROW sendo uma linha do .txt\n",
        "dfExemplo4 =spark.read.text('/content/sample_data/sherlock.txt')\n",
        "\n",
        "\n",
        "#coluna value (nome default) do Dataframe para python list[]\n",
        "listLivro = list(dfExemplo4.select(\"value\").toPandas()[\"value\"])\n",
        "\n",
        "\n",
        "#Formata list[linhaTxt1,...,linhaTxtN]\n",
        "#   para list[palavra1,...,palavraN]\n",
        "def listPalavras(listLivro):\n",
        "  resultado = []\n",
        "  for paragrafo in listLivro:\n",
        "   [resultado.append(palavra) for palavra in paragrafo.split()]\n",
        "  return resultado    \n",
        "\n",
        "#Conta ocorrencias com collections Counter\n",
        "contaPalavra1 = Counter(listPalavras(listLivro))\n",
        "\n",
        "#TOP 10\n",
        "listaTOP10 = contaPalavra1.most_common(10)\n",
        "listaTOP10"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxDQIJQHX5bj"
      },
      "source": [
        "# Lista -> rdd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmV2jIchTxVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ece174d-a729-4a80-dcfe-f1008e8cdb00"
      },
      "source": [
        "#python list -> spark dataframe\n",
        "rdd = spark.sparkContext.parallelize(listaTOP10)\n",
        "\n",
        "rdd.take(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fAiu_jAX-cv"
      },
      "source": [
        "# Lista -> Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx_CMJ_rYN0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62ddacd-b810-4858-e356-b01503a597f6"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "#novo schema\n",
        "schema = StructType([\n",
        "    StructField('palavra', StringType(), True),\n",
        "    StructField('qnt', StringType(), True)\n",
        "])\n",
        "#monta df\n",
        "df = spark.createDataFrame(listaTOP10,schema)\n",
        "df.show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|palavra| qnt|\n",
            "+-------+----+\n",
            "|    the|5412|\n",
            "|    and|2794|\n",
            "|     of|2724|\n",
            "|     to|2702|\n",
            "|      a|2575|\n",
            "|      I|2533|\n",
            "|     in|1706|\n",
            "|   that|1557|\n",
            "|    was|1361|\n",
            "|    his|1096|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4IVHLZoZFBn"
      },
      "source": [
        "# Dataframe -> .txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y28qY1y8VAtu"
      },
      "source": [
        "#Concatena colunas selecionadas e cria novo df com apenas 1 coluna\n",
        "df.createOrReplaceTempView('tabelaConc')\n",
        "df1coluna = spark.sql(\"\"\"SELECT CONCAT(palavra, ' ', qnt) palavra_e_qnt FROM tabelaConc\"\"\")\n",
        "spark.catalog.dropTempView('tabelaConc')\n",
        "\n",
        "#salva df em .txt\n",
        "df1coluna.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save('/content/sample_data/OUTPUT-TOP10sherlock')\n"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}